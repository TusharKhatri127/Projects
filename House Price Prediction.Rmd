---
title: "Prediction of Price Per Unit for Houses"
author: "Tushar Khatri"
date: "31/08/2022"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Context 
```
This project was made because we were intrigued and  we wanted  to  gain  hands-on  experience with the Regression Project .
```

# Objective

```
Addressed the assumption regression model such as heteroscedasity , multicollinearity , normality of residual.   
Fitted Multiple Regression Model on House Price dataset using the set of significant predictor variables.
Calculating Model fit to check whether model fits data well or not 

```

# Problem Statement
```
Create a Regression Model to  estimate  the  price  of  houses per unit area.

```

# Data
```
The  data  is  the  most  important  aspect  of  a  assignment, to which  special attention    should  be paid.  Indeed,  the  data  will  heavily  affect  the  findings depending  on  where  we  found  them,  how  they  are presented, if  they are consistent,if there is  an outlier,  and so on. To obtain, clean, and convert the data, many sub steps are required. We  will  go  through  these  steps  to  understand how  they've  been  used  in  my  project 

```
# Dataset 
```
Data set : https://www.kaggle.com/datasets/quantbruce/real-estate-price-prediction

```


# Importing Library
```{r}
library(readxl)

library(dplyr)

library(moments)

library(ggplot2)

library(psych)
```

# Reading data using read.csv
```{r}
data<-read.csv("C:/Users/TUSHAR/Downloads/PROJECT  (CV) EXCEL SHEET/Real estate.csv")
print(head(data))
```
# Exploratory Data Analysis

##### Dropping some columns which will be of no use.
```{r}
data = data[-c(1,6,7)]
print(head(data))
```

##### Renaming Columns

```{r}
colnames(data)<-c("transaction_date","house_age","distance_MRT_station","num_conv_stores",
                  "price_perunit_area","Bedroom.","Bathroom." )
```

```{r}
data$Bedroom.[data$price_perunit_area<=30]=1
data$Bedroom.[data$price_perunit_area>30 & data$price_perunit_area<=55]=2
data$Bedroom.[data$price_perunit_area>55 & data$price_perunit_area<=80]=3
data$Bedroom.[data$price_perunit_area>80]=4

data$Bathroom.[data$price_perunit_area<=45]=1
data$Bathroom.[data$price_perunit_area>45 & data$price_perunit_area<=80]=2
data$Bathroom.[data$price_perunit_area>80]=3

```

##### Checking Null Values
```{r}
lapply(data,is.null)

#No null values found in any coloumn of the Dataset
```


### Data type and Data Description of Coloumn

```
Transaction_Date:         This coloumn gives the year of the transaction.
House_Age :               This coloumn represnets the age of House.
Distance_MRT_station :    This coloumn describes about the Distance of the property 
                          from the MRT station
Num_Conv_Stores:          This coloumn gives the number of convienience store near 
                          that house.
Price_perunit_area :      This coloumn describes about the price per unit of that 
                          property.
Bedroom:                  This coloumn describes about the number of bedroom in that house.
Bathroom:                 This coloumn describes about the number of bathroom in that house.

```

### Data Structure
```{r}

str(data)

```

### Data Cleaning 

#1.) Extracting useful info from "transaction_date" column. Like Year, Quarter.

#2.) Transaction date column has 2 parts Year.MonthCode

#3.) Each month equals to 83.33 units additional to their previous month.

#4.) Eg-> 2013.250 means -> Year is 2013 and month is 250/83.33 (i.e 3rd)

#5.) Converting Transaction month in the form of Quarters.

```{r}
data$transaction_year<-as.numeric(substr(data$transaction_date,1,4))
data$transaction_month<-as.numeric((substr(data$transaction_date*1000,5,7)))
data$transaction_month[data$transaction_month<=250]=1
data$transaction_month[data$transaction_month<=500 & data$transaction_month>250]=2
data$transaction_month[data$transaction_month<=750 & data$transaction_month>500]=3
data$transaction_month[data$transaction_month>=750]=4

print(head(data))
```

# Verifying the assumption of Regression

### Histogram Plots

```{r}
library(ggplot2)
p<-ggplot(data, aes(x=price_perunit_area)) + 
  geom_histogram(color="black", fill="skyblue")
p
```

### Interpretation
```
1) From the plot we can see it is approximately normal distributed and have unique mode
2) A the values of price per unit area increases count also increases uoto certain level then decreases after certain point.
having maximum frequency around 42 approximately.
```
```{r}
p2<-ggplot(data, aes(x=house_age)) + 
  geom_histogram(color="black", fill="skyblue")
p2
```
### Interpretation
```
1) From the plot we can see it doesn't have unique mode.
2) From the plot we can see it not distributed uniformly according to the age variable
```
```{r}
p3<-ggplot(data, aes(x=distance_MRT_station)) + 
  geom_histogram(color="black", fill="skyblue")
p3
```
### Interpretation
```
1) From the plot we can see it is skewed towards right showing as the distance from mrt station icreases the value of count 
  variables go on decreasing.
2) After certain distance >5000 we can see the number of houses are very low as compared to before that showing 
   negative correlation between distance and count variable.
```
```{r}
p4<-ggplot(data, aes(x=num_conv_stores)) + 
  geom_histogram(color="black", fill="skyblue")
p4
```
```{r}
p5<-ggplot(data, aes(x=Bedroom.)) + 
  geom_histogram(color="black", fill="skyblue")
p5
```
### Interpretation
```
1) From the plot we can see the count of houses haveing 2 bedroom is maximum as comapred to the houses having 1 and 3 bedroom.
2) From the plot we can interpret than mode is 2 .
```
```{r}
p6<-ggplot(data, aes(x=Bathroom.)) + 
  geom_histogram(color="black", fill="skyblue")
p6
```
```{r}
p7<-ggplot(data, aes(x=transaction_year)) + 

  geom_histogram(color="black", fill="skyblue")
p7

```

# Scatter Plots
##### These Plots help to explain the values and how they are scattered
```{r}
plot(data$distance_MRT_station,data$price_perunit_area,col="skyblue",
     xlab="distance_MRT_station",ylab="price_perunit_area")
```
### Interpretation
```
1) From the scatter plot we can see that as distance from MRT station icreases price per unit decreases for houses.
2) We can see there is oulier showing price per unit area is 120 when distance is closest to station.
```
```{r}
plot(data$num_conv_stores,data$price_perunit_area,col="skyblue",xlab="num_conv_stores",ylab="price_perunit_area")
```
### Interpretation
```
1) From the plot we can see that as the number of conv store increaes the variability between the price of houses decreases 
2) And as the number of stores goes on increasing price also increases showing positive correlation (relationship) between theses two variable
```
```{r}
plot(data$house_age,data$price_perunit_area,col="skyblue",xlab="House age",ylab="price_perunit_area")
```

```{r}
plot(data$Bedroom.,data$price_perunit_area,col="skyblue",xlab="No of Bedroom",ylab="price_perunit_area")
```

```{r}
plot(data$Bathroom.,data$price_perunit_area,col="skyblue",xlab="No of Bathroom",ylab="price_perunit_area")
```

# Boxplot
#### Looking for Outliers
```{r}
par(mfrow=c(2,2))
boxplot(data$house_age,main = "Outliers in  boxplots for House age ",col = "orange",
        border = "brown",horizontal = T,notch = T)
```

### Interpretation

```
1) From the boxplot in house age we can see that median is shifted more towards first quartile showing highly skewed data,
2) We can there there is almost no outlier in house age data.
3) It is skewed towards right 
```
```{r}
boxplot(data$distance_MRT_station,main = "Outliers in boxplots for distance_MRT_station "
        ,col = "orange",border = "brown",horizontal = T,notch = T)

```

### Interpretation

```
1) From the boxplot in distance mrt staion we can see that median is shifted more towards first quartile showing highly skewed data,
2) We can there there are many ouliers in house age data.
3) It is highly skewed towards right.

```
```{r}
boxplot(data$num_conv_stores,main = "Outliers in boxplots for num_conv_stores",
        col = "orange",border = "brown",horizontal = T,notch = T)
```

### Interpretation

```
1) From the boxplot in distance mrt staion we can see that median is shifted more towards third quartile showing highly skewed data,
2) We can there there are almost no outliers in house age data.
3) It is highly skewed towards left.

```

```{r}
boxplot(data$price_perunit_area,main = "Outliers in boxplots for price_perunit_area",
        col = "orange",border = "brown",horizontal = T,notch = T)
```


# Fitting Multiple Regression Model 
```
# In Mutiple Regression, the Null Hypothesis is that the coefficients associated with the variables is equal to zero. 
# The alternate hypothesis is that the coefficients are not equal to zero 
# (i.e. there exists a relationship between the independent variable in question and the dependent variable).
# P value has 3 stars which means x is of very high statistical significance.
# P value is less than 0. Genraaly below 0.05 is considered good.
# R-Squared tells us is the proportion of variation in the dependent (response) variable 
  that has been explained by this model.
```

```{r}
model = lm(data$price_perunit_area~data$house_age+data$distance_MRT_station+
             data$num_conv_stores+data$Bedroom.+data$Bathroom.,data) 
#Create the Multiple regression model
print(model)
```

# Checking Heteroscedasticty 
```{r}
library(lmtest)
bptest(model)

```
### Interpretation 
```
Since p-value of the Breush Pagan test is less than 0.01 we reject the null hypothesis at 1% level of significance
```
# Checking for Dependence of Residual Using Durbin Watson test
```{r}
library(car)
durbinWatsonTest(model)
```
### Interpretation
```
From the output we can see that the test statistic is 1.91 and the corresponding p-value is 0.42. Since this p-value is less than 0.42, we can accept the null hypothesis and conclude that the residuals in this regression model are not autocorrelated.
```
# Using Pricipal Component analysis to detect Multicollinearity

```{r}
results <- prcomp(data, scale = TRUE)

#reverse the signs
results$rotation <- -1*results$rotation

#display principal components
results$rotation
```

```{r}
#reverse the signs of the scores
results$x <- -1*results$x

#display the scores
head(results$x)
```
# Birplot to Display the results
```{r}
biplot(results, scale = 0)
```
### calculate total variance explained by each principal component
```{r}
results$sdev^2 / sum(results$sdev^2)
```
# Interpretation from Principal Component Analysis
```
From the results we can observe the following:

The first principal component explains 38.8% of the total variance in the dataset.
The second principal component explains 26.3% of the total variance in the dataset.
The third principal component explains 12.9% of the total variance in the dataset.
The fourth principal component explains 7.02% of the total variance in the dataset.
```

```
Thus, the first two principal components explain a majority of the total variance in the data.

This is a good sign because the previous biplot projected each of the observations from the original data onto a scatterplot that only took into account the first two principal components.
```

### Review the summary of the model fitted
```{r}
model = lm(data$price_perunit_area~data$house_age+data$distance_MRT_station+
             data$num_conv_stores+data$Bedroom.+data$Bathroom.,data) #Create the linear regression
print(model)
```


# Identifying Set of Significant Predictor Variables
### Summary Description
```{r}
model = lm(data$price_perunit_area~data$house_age+data$distance_MRT_station+
             data$num_conv_stores+data$Bedroom.+data$Bathroom.,data) #Create the linear regression
summary(model)
```

### Looking at some key statistics from the summary

```
The values we are concerned with are -

1)The coefficients and significance (p-values)
2)R-squared
3)F statistic and its significance

###
1) The p-value for house age is 0.11580 In other words, there’s 11.58% chance
that this predictor is not meaningful for the regression.

2) The p-value for Distance MRT station is < 8.95e-14 A very small value means that 
age is probably an excellent addition to our model.

3) The p-value for num_conv_stores  is 0.137249.In other words, there’s 13.72% chance
that this predictor is not meaningful for the regression.

4) The p-value for No of Bedroom  is < 2e-16. A very small value means that 
age is probably an excellent addition to our model.

5) The p-value for the number of Bathroom is < 2e-16. A very small value means that 
age is probably an excellent addition to our model.


###
2) R - squared is 0.8989
Meaning that  89.89% of the variance in House price prediction is explained by explainatory variable

###
3) F statistic has a very low p value (practically low)
Meaning that the model fit is statistically significant, and the explained variance isn't purely by chance.

###
4) Transacation Year and Transaction month hav every high p value showing they are not meaningful to be Predictor Variable.


So far we have seen how to build a linear regression model using the whole dataset. If we 
build it that way, there is no way to tell how the model will perform with new data. So 
the preferred practice is to split your dataset into a 80:20 sample (training:test), then,
build the model on the 80% sample and then use the model thus built to predict the dependent
variable on test data.

Doing it this way, we will have the model predicted values for the 20% data (test) as well 
as the actuals (from the original dataset). By calculating accuracy measures (like min_max
accuracy) and error rates (MAPE or MSE), we can find out the prediction accuracy of the model.

```

##  Value of Adjusted R^2 from model is 0.897

# Buiding Model on Training Data

### Traning Dataset
```{r}
set.seed(100) 
trainingRowIndex <- sample(1:nrow(data), 0.8*nrow(data))
trainingData <- data[trainingRowIndex, ]  # model training data
testData  <- data[-trainingRowIndex, ]   # test data
```

```{r}
set.seed(100) 
trainingRowIndex <- sample(1:nrow(data), 0.8*nrow(data))
trainingData <- data[trainingRowIndex, ]  # model training data
testData  <- data[-trainingRowIndex, ]   # test data
lmMod <- lm(price_perunit_area ~ house_age + distance_MRT_station +num_conv_stores+Bedroom.+Bathroom.,data=trainingData)  
# build the model
distPred <- predict(lmMod, testData)  # predict price per unit area



```

##### Interpretation
```
1) We have applied t-test to know whether the particular variable is correlated with 
 variable which has to be predicted or not(response variable).
 
2)From the model summary, the model p value and predictor’s p value are less than the 
significance level, so we know we have a statistically significant model.
```
# Regression Plots

### Plot of Line of Best fits
```{r}
par(mfrow=c(3,3))
plot(y=data$price_perunit_area, x=data$house_age,xlab="house_age")
abline(lm(data$price_perunit_area~data$house_age),col="blue")

plot(y=data$price_perunit_area, x=data$distance_MRT_station,xlab="distance_MRT_station")
abline(lm(data$price_perunit_area~data$distance_MRT_station),col="blue")

plot(y=data$price_perunit_area, x=data$num_conv_stores,xlab="num_conv_stores")
abline(lm(data$price_perunit_area~data$num_conv_stores),col="blue")

plot(y=data$price_perunit_area, x=data$Bedroom.,xlab="No of Bedroom")
abline(lm(data$price_perunit_area~data$Bedroom.),col="blue")

plot(y=data$price_perunit_area, x=data$Bathroom.,xlab="No of Bedroom")
abline(lm(data$price_perunit_area~data$Bathroom.),col="blue")

plot(y=data$price_perunit_area, x=data$transaction_year,xlab="Transaction Year")
abline(lm(data$price_perunit_area~data$transaction_year),col="blue")

plot(y=data$price_perunit_area, x=data$transaction_month,xlab="Transaction Month")
abline(lm(data$price_perunit_area~data$transaction_month),col="blue")

```

# Checking Normality of Error Terms using Q-Q Plot
### Residual Plot
```{r}
res <- resid(model)

# Looking for patterns in the residuals


plot(fitted(model), res)
abline(0,0)
qqnorm(res)
qqline(res) 
```

##### Interpretation
```
1)We have produced a Q-Q plot in plot 1, which is useful for determining if the residuals
follow a normal distribution. If the data values in the plot fall along a roughly straight
line at a 45-degree angle then the data is normally distributed.Here it is normally distributed

```

### Density plot of Residual
```{r}
# We need to check if the error terms are also normally distributed (which is infact, one of

# the major assumptions of linear regression), let us plot the histogram of the error terms 

# and see what it looks like.

res <- resid(model)
p<-ggplot(data, aes(x=res)) + 
  geom_histogram(aes(y=..density..),color="black", fill="blue")+geom_density()
p
sd(res)
mean(res)
 
```
##### Interpretation
```
1) From the residual plot we can see the density curve of the residual is approximately        normally distributed.
2) It is symmtric about 0 (approximately) so it can be said it is normally distributed 
approximately with mean 0 and standard deviation approximately equals to  4.325714/

```

# Conclusions
```
With  several  characteristics, the  suggested  method predicts  the  property  price. We dvided data into 80:20 to test 
Data on remaining data(20%) to  get  the  best  model.  Value of R^2 for this model was found to be descent showing model
fits well to the data and explains greatest variablility in data.

```

# Insights of the analysis done on above data
```
1) Analyzed the data to detect problems such as heteroscedasticity, dependence & non-normality of errors 
2) Fitted Multiple Regression Model on House Price dataset using the set of significant predictor variables
3)  Addressed multicollinearity using Principal Component Analysis and achieved adjusted R2 = 0.897

```


